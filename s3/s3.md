## S3

### facts about s3 storage

- files can be from 0 bytes to 5 TB
- files are stored in buckets
- one important idea is that s3 is a universal namespace. That is bucket names must be unique globally. This is because for each bucket that is created, s3 actually provides a url that a user may connect. The typical way bucket urls are formed is: `https://s3-eu-west-1.amazonaws.com/<bucket_name>`. Here the first part of the url is essentially the region name. 
- when we upload a file to s3, we get a http 200 status code. 
    
### Data consistency model for s3

- read after write consistency for PUTS of new objects. This means that when we upload a file to any s3 bucket, immediately after uploading if we try to read the file we will be able to read it.
- eventual consistency for overwrite PUTS and DELETES(can take some time to propagate). If we update a file that was already present in an s3 bucket and we immediately try to read it, the update may take some time to propagate. similarly for delete.

s3 is object based. an object in s3 consists of the following components:
- Key(This is simply the name of the object)
- Value(This is simply the data made up of a sequence of bytes)
- VersionID(important for versioning)
- Metadata(data about data, could be tags and stuff like when it was uploaded, the size of the data in the file and the like)

### subresources

- access control lists: we can put individual permissions on our files.
- torrent: for torrenting

- s3 is built for 99.99% availability
- amazon actually gurantees 99.9% availability. This is actually the SLA(service level agreement)
- amazon gurantees 99.999999999% durability of files. This is sometimes called the 11 9s rule. basically it means that there is that percent chance that we are going to not lose a file.
- storage classes are available. tiered storage available.
- lifecycle management: we can do stuff like if we say that a file is x days old, we can move that file into a different storage tier and eventually move that file into glacier or archive it into galcier.
- versioning: versioning in files.
- encryption: encrypting a file in s3 is also possible. 
    - client side encryption
    - server side encryption
        - amazon s3 managed keys
        - KMS
        - customer provided keys(where the user basically provides the keys)
- secure data using access control lists. these can access who access individual files
- bucket policies: control who can access a bucket. This is obviosuly at the bucket level. In general control access to buckets is done using either access control lists or bucket policies.

### S3 Storage tiers or storage classes

- **S3 standard:** 99.99% availability and 11 99s durability. stored **redundantly** across **mulitple devices(disks)** in **multiple facilities(AZs)** and is designed to systain the loss of 2 facilities concurrently.
- **S3 IA:** s3 infrequently accessed. This is for data that is accessed less frequently however you still need the data to be accessed rapidly. it has lower fee than s3 but you are charged a retrieval fee. Once again stored in multiple devices across multiple AZs.
- **S3 One zone-IA**This is the same as IA however data is only stored in one AZ. This has even a lower cost than normal IA. 
- **Glacier**: cheapest storage and is used for data archival only. It again comes in 3 models: expedidited, standard and bulk. in case of expedidited the data retrieval time would be a few minutes, for standard it is 3-5 hrs and for bulk it is 5-12 hrs. glacier is never meant for data that needs to be retrieved

Note: other than s3 standard all other models have a retrieval fee. 

### s3 charges

- **storage(per gb)**
- **requests(num of requests)**
- **storage management pricing:** when we are storing something to s3, we have the ability to tag it. actually tagging also costs
- **data transfer pricing:** cross region replication. basically if we are replicating our data from one region to another.
- **transfer acceleration:** Transfer acceleration enables fast, easy and secure transfers of files over long distances between our end users and your s3 buckets. Transfer acceleration takes advantage of amazon's cdb cloudfront in order to cache data in amazon's edge locations for fast retrieval. Obviously it comes at a price. Whenever a user wants to upload a file to an s3 bucket, he/she would actually upload to an edge location and from the edge location the data would get transferred to the original bucket using amazon's backbone network.

### Versioning

if versioning is enabled it cannot be disabled. after enabling versioning, we can only suspend it, we cannot disable it. Since in case of s3, we are charged for storage, it is important to understand when to use versioning and when not to use it since versioning would mean that s3 would actually store multiple copies of the fies. So if its a big file and its prone to change, enabling versioning on such a file might become expensive. 

Deleting object versions:
When an object is deleted simply using the object name, the object is not actually permanently removed from s3. A delete marker is actually placed in the object and that becomes the object's new version. It is only when we delete a version of the object is the object permanently deleted.

It is also possible to put an MFA layer of security on deletion of an object.

### Cross Region Replication

Cross region replication means that we want to replicate an entire bucket or specific folders of a bucket from one bucket(in one region) to another bucket(in another region). One thing to note here is that for cross region replication, versioning must be enabled in both buckets.

When cross region replication is enabled, it is only the new objects that are uploaded after cross region replication rules are set that are going to be replicated. The old objects are going to remain as it is. Also when an old object is updated, it is going to be replicated into the new bucket. 
In case of deletion of objects in the source bucket i.e. placing a delete marker in an object, the delete marker is automatically placed in the replicated bucket ordestination bucket as well. One important thing to note however is that if the delete marker itself is also deleted, the object is not however deleted in the replicated bucket. As a general rule for deletion of versions, if a specific version is deleted in the source bucket, the corresponding version of the object is actually not deleted in the destination bucket.

In order to easily monitor the replication status of s3 objects, aws offers the cross region replication monitor(crr monitor) solution. This solution automatically checks the replication status of amazon s3 objects across different aws regions, providing near real time metrics as well as failure notifications to help customers proactively identify failures and troubleshoot problems. 
CRR Monitor is not a service. The implementation of a CRR monitor involves architecting a solution using a couple of different aws services. AWS provides a guide that can help us put together the CRR monitor. For more information check out [https://aws.amazon.com/answers/infrastructure-management/crr-monitor/](https://aws.amazon.com/answers/infrastructure-management/crr-monitor/)

Cross region translation cannot be done for buckets in the same region(obviously).

For copying the existing contents of one bucket into another bucket we can use `awscli`
So once the aws cli is configured we can use the following command to copy the contents of one bucket into another bucket:
`aws cp --recursive s3://<source-bucket-name> s3://<dest-bucket-name>`

### Lifecycle management

Lifecycle management comes into the picture if we want to store specific data in s3 for a certain period of time and then transfer it to a different tier (may be s3-IA) when the data is not that relevant and we know it will be infrequently accessed. We can also have the data actually moved to (archived) glacier for even cheaper storage if the data is not relevant for retrieval. 
Lifecycle rules can be used with or without versioning.
Can be applied to current versions as well as pervious versions.
There is an expiration that can also be set for the objects.

